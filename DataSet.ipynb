{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "def read_csv(filename, hasHeader=False):\n",
    "    data = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        if (hasHeader):\n",
    "            next(reader, None)\n",
    "            \n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train items 3344613\n",
      "Item example ['1', '81', 'Продам Камаз 6520', 'Продам Камаз 6520 20 тонн', '1064094, 5252822, 6645873, 6960145, 9230265', '{\"Вид техники\":\"Грузовики\"}', '300000.0', '648140', '', '64.686946', '30.815924']\n"
     ]
    }
   ],
   "source": [
    "items = read_csv('data/ItemInfo_train.csv', hasHeader=True)\n",
    "print('Train items', len(items))\n",
    "print('Item example', items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COL_ITEM_ID=0\n",
    "COL_CATEGORY_ID=1\n",
    "COL_TITLE=2\n",
    "COL_DESCRIPTION=3\n",
    "COL_IMAGES=4\n",
    "COL_JSON=5\n",
    "COL_PRICE=6\n",
    "COL_LOCATION=7\n",
    "COL_METRO=8\n",
    "COL_LAT=9\n",
    "COL_LON=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_items = dict()\n",
    "for item in items:\n",
    "    map_items[item[COL_ITEM_ID]] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs 20000\n",
      "Example pair ['1', '4112648', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "pairs = read_csv('data/ItemPairs_train.csv', hasHeader=True)\n",
    "pairs = pairs[:20000]\n",
    "print('Train pairs', len(pairs))\n",
    "print('Example pair', pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories 52\n"
     ]
    }
   ],
   "source": [
    "categories = read_csv('data/Category.csv')\n",
    "print('Categories', len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_parent_category = dict()\n",
    "for category in categories:\n",
    "    map_parent_category[category[0]] = category[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locations 3449\n"
     ]
    }
   ],
   "source": [
    "locations = read_csv('data/Location.csv', hasHeader=True)\n",
    "print('Locations', len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_regions = dict()\n",
    "for location in locations:\n",
    "    map_regions[location[0]] = location[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_hashes = {}\n",
    "\n",
    "for i in range(10):\n",
    "    data = read_csv('processed_data/hashes' + str(i) + '.csv')\n",
    "    for row in data:\n",
    "        map_hashes[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize)\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    try:\n",
    "        tfidf = vectorizer.fit_transform([text1, text2])\n",
    "        return ((tfidf * tfidf.T).A)[0,1]\n",
    "    except:\n",
    "        return 1.0 if text1 == text2 else 0.0\n",
    "\n",
    "def same_title(item1, item2):\n",
    "    return cosine_sim(item1[COL_TITLE], item2[COL_TITLE])\n",
    "\n",
    "def same_description(item1, item2):\n",
    "    return cosine_sim(item1[COL_DESCRIPTION], item2[COL_DESCRIPTION])\n",
    "\n",
    "def number_of_same_words_title(item1, item2):\n",
    "    title1 = set(normalize(item1[COL_TITLE]))\n",
    "    title2 = set(normalize(item2[COL_TITLE]))\n",
    "    \n",
    "    return len(title1.intersection(title2))\n",
    "\n",
    "def number_of_same_words_title_rel(item1, item2):\n",
    "    title1 = set(normalize(item1[COL_TITLE]))\n",
    "    title2 = set(normalize(item2[COL_TITLE]))\n",
    "    m = min(len(title1), len(title2))\n",
    "    if m == 0:\n",
    "        return 0\n",
    "    \n",
    "    return len(title1.intersection(title2)) / m\n",
    "\n",
    "def number_of_same_words_desc(item1, item2):\n",
    "    title1 = set(normalize(item1[COL_DESCRIPTION]))\n",
    "    title2 = set(normalize(item2[COL_DESCRIPTION]))\n",
    "    \n",
    "    return len(title1.intersection(title2))\n",
    "\n",
    "def number_of_same_words_desc_rel(item1, item2):\n",
    "    title1 = set(normalize(item1[COL_DESCRIPTION]))\n",
    "    title2 = set(normalize(item2[COL_DESCRIPTION]))\n",
    "    m = min(len(title1), len(title2))\n",
    "    if m == 0:\n",
    "        return 0\n",
    "    \n",
    "    return len(title1.intersection(title2)) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def number_of_same_images(item1, item2):\n",
    "    imgs1 = item1[COL_IMAGES].split(',')\n",
    "    imgs2 = item2[COL_IMAGES].split(',')\n",
    "    imgs1 = set(filter(bool, [x.strip() for x in imgs1]))\n",
    "    imgs2 = set(filter(bool, [x.strip() for x in imgs2]))\n",
    "    \n",
    "    count = 0\n",
    "    for img1 in imgs1:\n",
    "        for img2 in imgs2:\n",
    "            try:\n",
    "                if map_hashes[img1+'.jpg'] == map_hashes[img2+'.jpg']:\n",
    "                    count += 1\n",
    "                    break\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "                \n",
    "    minlen = min(len(imgs1), len(imgs2))\n",
    "    \n",
    "    if minlen == 0:\n",
    "        return (count, 0)\n",
    "    \n",
    "    return (count, count / minlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def same_category(item1, item2):\n",
    "    cat1 = item1[COL_CATEGORY_ID]\n",
    "    cat2 = item2[COL_CATEGORY_ID]\n",
    "    \n",
    "    return 1 if cat1 == cat2 else 0\n",
    "\n",
    "def same_parent_category(item1, item2):\n",
    "    cat1 = item1[COL_CATEGORY_ID]\n",
    "    cat2 = item2[COL_CATEGORY_ID]\n",
    "    \n",
    "    par_cat1 = map_parent_category[cat1]\n",
    "    par_cat2 = map_parent_category[cat2]\n",
    "    \n",
    "    return 1 if par_cat1 == par_cat2 else 0\n",
    "\n",
    "def same_price(item1, item2):\n",
    "    price1 = item1[COL_PRICE]\n",
    "    price2 = item2[COL_PRICE]\n",
    "    \n",
    "    return 1 if price1 == price2 else 0\n",
    "\n",
    "def safe_parse(str):\n",
    "    try:\n",
    "        return float(item1[COL_PRICE].replace(',', ''))\n",
    "    except:\n",
    "        return 0.0\n",
    "    \n",
    "def price_diff(item1, item2):\n",
    "    price1 = safe_parse(item1[COL_PRICE])\n",
    "    price2 = safe_parse(item2[COL_PRICE])\n",
    "    \n",
    "    diff = math.fabs(price1 - price2)\n",
    "    avg_price = min(price1, price2)\n",
    "    \n",
    "    if avg_price == 0:\n",
    "        return (diff, 0)\n",
    "    \n",
    "    return (diff, diff / avg_price)\n",
    "\n",
    "def same_lat(item1, item2):\n",
    "    lat1 = item1[COL_LAT]\n",
    "    lat2 = item2[COL_LAT]\n",
    "    \n",
    "    return 1 if lat1 == lat2 else 0\n",
    "\n",
    "def same_lon(item1, item2):\n",
    "    lon1 = item1[COL_LON]\n",
    "    lon2 = item2[COL_LON]\n",
    "    \n",
    "    return 1 if lon1 == lon2 else 0\n",
    "\n",
    "def same_location(item1, item2):\n",
    "    location1 = item1[COL_LOCATION]\n",
    "    location2 = item2[COL_LOCATION]\n",
    "    \n",
    "    return 1 if location1 == location2 else 0\n",
    "\n",
    "def distance_between_coordinates(item1, item2):\n",
    "    lat1 = float(item1[COL_LAT])\n",
    "    lat2 = float(item2[COL_LAT])\n",
    "    lon1 = float(item1[COL_LON])\n",
    "    lon2 = float(item2[COL_LON])\n",
    "    \n",
    "    point1 = (lon1, lat1)\n",
    "    point2 = (lon2, lat2)\n",
    "    \n",
    "    from geopy.distance import vincenty\n",
    "    return vincenty(point1, point2).miles\n",
    "\n",
    "def same_region(item1, item2):\n",
    "    location1 = item1[COL_LOCATION]\n",
    "    location2 = item2[COL_LOCATION]\n",
    "    \n",
    "    region1 = map_regions[location1]\n",
    "    region2 = map_regions[location2]\n",
    "    \n",
    "    return 1 if region1 == region2 else 0\n",
    "\n",
    "def same_metro(item1, item2):\n",
    "    metro1 = item1[COL_METRO]\n",
    "    metro2 = item2[COL_METRO]\n",
    "    \n",
    "    return 1 if metro1 == metro2 else 0\n",
    "\n",
    "def get_features(item1, item2, label):\n",
    "    fx = []\n",
    "    fx.append(same_category(item1, item2))\n",
    "    fx.append(same_parent_category(item1, item2))\n",
    "    \n",
    "    (price_diff_abs, price_diff_rel) = price_diff(item1, item2)\n",
    "    fx.append(price_diff_abs)\n",
    "    fx.append(price_diff_rel)\n",
    "    \n",
    "    fx.append(same_lat(item1, item2))\n",
    "    fx.append(same_lon(item1, item2))\n",
    "    fx.append(same_location(item1, item2))\n",
    "    fx.append(distance_between_coordinates(item1, item2))\n",
    "    \n",
    "    fx.append(same_region(item1, item2))\n",
    "    fx.append(same_metro(item1, item2))\n",
    "    \n",
    "#     fx.append(same_title(item1, item2))\n",
    "    fx.append(number_of_same_words_title_rel(item1, item2))\n",
    "\n",
    "#     fx.append(same_description(item1, item2))\n",
    "    fx.append(number_of_same_words_desc_rel(item1, item2))\n",
    "    \n",
    "    (img_sim_abs, img_sim_rel) = number_of_same_images(item1, item2)\n",
    "    fx.append(img_sim_abs)\n",
    "    fx.append(img_sim_rel)\n",
    "    \n",
    "    return (fx, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 ms, sys: 8.03 ms, total: 28.4 ms\n",
      "Wall time: 325 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 1, 0.0, 0.0, 1, 1, 1, 0.0, 1, 1, 1.0, 1.0, 4, 1.0], 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1 = map_items[pairs[0][0]]\n",
    "item2 = map_items[pairs[0][1]]\n",
    "\n",
    "%time get_features(item1, item2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 1, 0.0, 0.0, 1, 1, 1, 0.0, 1, 1, 1.0, 1.0, 4, 1.0], 1)\n"
     ]
    }
   ],
   "source": [
    "def get_train_data(items):\n",
    "    data = []\n",
    "    for pair in items:\n",
    "        item1 = map_items[pair[0]]\n",
    "        item2 = map_items[pair[1]]\n",
    "        label = int(pair[2])\n",
    "\n",
    "        xy = get_features(item1, item2, label)\n",
    "        data.append(xy)\n",
    "        \n",
    "    return data\n",
    "\n",
    "train = get_train_data(pairs)\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "# datas = {}\n",
    "# def get_train_data_worker(i, n):\n",
    "#     data = []\n",
    "#     start = len(pairs) / n * i\n",
    "#     end = len(pairs) / n * (i+1)\n",
    "#     end = min(end, len(pairs))\n",
    "#     job = pairs[start:end]\n",
    "#     for pair in job:\n",
    "#         item1 = map_items[pair[0]]\n",
    "#         item2 = map_items[pair[1]]\n",
    "#         label = int(pair[2])\n",
    "\n",
    "#         xy = get_features(item1, item2, label)\n",
    "#         data.append(xy)\n",
    "#     datas[i] = data\n",
    "    \n",
    "# def get_train_data_async():\n",
    "#     parts = 4\n",
    "    \n",
    "#     threads = []\n",
    "#     for i in range(parts):\n",
    "#         thread=Thread(target=get_train_data_worker, args=(i+1, parts))\n",
    "#         threads.append(thread)\n",
    "#         thread.start()\n",
    "    \n",
    "#     for thread in threads:\n",
    "#         thread.join()\n",
    "    \n",
    "#     data = []\n",
    "#     for key in datas:\n",
    "#         data = data + datas[key]\n",
    "        \n",
    "#     return data\n",
    "\n",
    "# train = get_train_data_async()\n",
    "# print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_data, test_data = train_test_split(train, test_size=0.20, train_size=0.80)\n",
    "\n",
    "train_x = [x[0] for x in train_data]\n",
    "train_y = [x[1] for x in train_data]\n",
    "\n",
    "test_x = [x[0] for x in test_data]\n",
    "test_y = [x[1] for x in test_data]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10\n",
    ")\n",
    "# classifier = SVC()\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "print(classifier.score(test_x, test_y))\n",
    "# print(classifier.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rfc.pkl',\n",
       " 'rfc.pkl_01.npy',\n",
       " 'rfc.pkl_02.npy',\n",
       " 'rfc.pkl_03.npy',\n",
       " 'rfc.pkl_04.npy',\n",
       " 'rfc.pkl_05.npy',\n",
       " 'rfc.pkl_06.npy',\n",
       " 'rfc.pkl_07.npy',\n",
       " 'rfc.pkl_08.npy',\n",
       " 'rfc.pkl_09.npy',\n",
       " 'rfc.pkl_10.npy',\n",
       " 'rfc.pkl_11.npy',\n",
       " 'rfc.pkl_12.npy',\n",
       " 'rfc.pkl_13.npy',\n",
       " 'rfc.pkl_14.npy',\n",
       " 'rfc.pkl_15.npy',\n",
       " 'rfc.pkl_16.npy',\n",
       " 'rfc.pkl_17.npy',\n",
       " 'rfc.pkl_18.npy',\n",
       " 'rfc.pkl_19.npy',\n",
       " 'rfc.pkl_20.npy',\n",
       " 'rfc.pkl_21.npy',\n",
       " 'rfc.pkl_22.npy',\n",
       " 'rfc.pkl_23.npy',\n",
       " 'rfc.pkl_24.npy',\n",
       " 'rfc.pkl_25.npy',\n",
       " 'rfc.pkl_26.npy',\n",
       " 'rfc.pkl_27.npy',\n",
       " 'rfc.pkl_28.npy',\n",
       " 'rfc.pkl_29.npy',\n",
       " 'rfc.pkl_30.npy',\n",
       " 'rfc.pkl_31.npy',\n",
       " 'rfc.pkl_32.npy',\n",
       " 'rfc.pkl_33.npy',\n",
       " 'rfc.pkl_34.npy',\n",
       " 'rfc.pkl_35.npy',\n",
       " 'rfc.pkl_36.npy',\n",
       " 'rfc.pkl_37.npy',\n",
       " 'rfc.pkl_38.npy',\n",
       " 'rfc.pkl_39.npy',\n",
       " 'rfc.pkl_40.npy',\n",
       " 'rfc.pkl_41.npy',\n",
       " 'rfc.pkl_42.npy',\n",
       " 'rfc.pkl_43.npy',\n",
       " 'rfc.pkl_44.npy',\n",
       " 'rfc.pkl_45.npy',\n",
       " 'rfc.pkl_46.npy',\n",
       " 'rfc.pkl_47.npy',\n",
       " 'rfc.pkl_48.npy',\n",
       " 'rfc.pkl_49.npy',\n",
       " 'rfc.pkl_50.npy',\n",
       " 'rfc.pkl_51.npy',\n",
       " 'rfc.pkl_52.npy',\n",
       " 'rfc.pkl_53.npy',\n",
       " 'rfc.pkl_54.npy',\n",
       " 'rfc.pkl_55.npy',\n",
       " 'rfc.pkl_56.npy',\n",
       " 'rfc.pkl_57.npy',\n",
       " 'rfc.pkl_58.npy',\n",
       " 'rfc.pkl_59.npy',\n",
       " 'rfc.pkl_60.npy',\n",
       " 'rfc.pkl_61.npy',\n",
       " 'rfc.pkl_62.npy',\n",
       " 'rfc.pkl_63.npy',\n",
       " 'rfc.pkl_64.npy',\n",
       " 'rfc.pkl_65.npy',\n",
       " 'rfc.pkl_66.npy',\n",
       " 'rfc.pkl_67.npy',\n",
       " 'rfc.pkl_68.npy',\n",
       " 'rfc.pkl_69.npy',\n",
       " 'rfc.pkl_70.npy',\n",
       " 'rfc.pkl_71.npy',\n",
       " 'rfc.pkl_72.npy',\n",
       " 'rfc.pkl_73.npy',\n",
       " 'rfc.pkl_74.npy',\n",
       " 'rfc.pkl_75.npy',\n",
       " 'rfc.pkl_76.npy',\n",
       " 'rfc.pkl_77.npy',\n",
       " 'rfc.pkl_78.npy',\n",
       " 'rfc.pkl_79.npy',\n",
       " 'rfc.pkl_80.npy',\n",
       " 'rfc.pkl_81.npy',\n",
       " 'rfc.pkl_82.npy',\n",
       " 'rfc.pkl_83.npy',\n",
       " 'rfc.pkl_84.npy',\n",
       " 'rfc.pkl_85.npy',\n",
       " 'rfc.pkl_86.npy',\n",
       " 'rfc.pkl_87.npy',\n",
       " 'rfc.pkl_88.npy',\n",
       " 'rfc.pkl_89.npy',\n",
       " 'rfc.pkl_90.npy',\n",
       " 'rfc.pkl_91.npy',\n",
       " 'rfc.pkl_92.npy',\n",
       " 'rfc.pkl_93.npy',\n",
       " 'rfc.pkl_94.npy',\n",
       " 'rfc.pkl_95.npy',\n",
       " 'rfc.pkl_96.npy',\n",
       " 'rfc.pkl_97.npy',\n",
       " 'rfc.pkl_98.npy',\n",
       " 'rfc.pkl_99.npy',\n",
       " 'rfc.pkl_100.npy',\n",
       " 'rfc.pkl_101.npy',\n",
       " 'rfc.pkl_102.npy',\n",
       " 'rfc.pkl_103.npy',\n",
       " 'rfc.pkl_104.npy',\n",
       " 'rfc.pkl_105.npy',\n",
       " 'rfc.pkl_106.npy',\n",
       " 'rfc.pkl_107.npy',\n",
       " 'rfc.pkl_108.npy',\n",
       " 'rfc.pkl_109.npy',\n",
       " 'rfc.pkl_110.npy',\n",
       " 'rfc.pkl_111.npy',\n",
       " 'rfc.pkl_112.npy',\n",
       " 'rfc.pkl_113.npy',\n",
       " 'rfc.pkl_114.npy',\n",
       " 'rfc.pkl_115.npy',\n",
       " 'rfc.pkl_116.npy',\n",
       " 'rfc.pkl_117.npy',\n",
       " 'rfc.pkl_118.npy',\n",
       " 'rfc.pkl_119.npy',\n",
       " 'rfc.pkl_120.npy',\n",
       " 'rfc.pkl_121.npy',\n",
       " 'rfc.pkl_122.npy',\n",
       " 'rfc.pkl_123.npy',\n",
       " 'rfc.pkl_124.npy',\n",
       " 'rfc.pkl_125.npy',\n",
       " 'rfc.pkl_126.npy',\n",
       " 'rfc.pkl_127.npy',\n",
       " 'rfc.pkl_128.npy',\n",
       " 'rfc.pkl_129.npy',\n",
       " 'rfc.pkl_130.npy',\n",
       " 'rfc.pkl_131.npy',\n",
       " 'rfc.pkl_132.npy',\n",
       " 'rfc.pkl_133.npy',\n",
       " 'rfc.pkl_134.npy',\n",
       " 'rfc.pkl_135.npy',\n",
       " 'rfc.pkl_136.npy',\n",
       " 'rfc.pkl_137.npy',\n",
       " 'rfc.pkl_138.npy',\n",
       " 'rfc.pkl_139.npy',\n",
       " 'rfc.pkl_140.npy',\n",
       " 'rfc.pkl_141.npy',\n",
       " 'rfc.pkl_142.npy',\n",
       " 'rfc.pkl_143.npy',\n",
       " 'rfc.pkl_144.npy',\n",
       " 'rfc.pkl_145.npy',\n",
       " 'rfc.pkl_146.npy',\n",
       " 'rfc.pkl_147.npy',\n",
       " 'rfc.pkl_148.npy',\n",
       " 'rfc.pkl_149.npy',\n",
       " 'rfc.pkl_150.npy',\n",
       " 'rfc.pkl_151.npy',\n",
       " 'rfc.pkl_152.npy',\n",
       " 'rfc.pkl_153.npy',\n",
       " 'rfc.pkl_154.npy',\n",
       " 'rfc.pkl_155.npy',\n",
       " 'rfc.pkl_156.npy',\n",
       " 'rfc.pkl_157.npy',\n",
       " 'rfc.pkl_158.npy',\n",
       " 'rfc.pkl_159.npy',\n",
       " 'rfc.pkl_160.npy',\n",
       " 'rfc.pkl_161.npy',\n",
       " 'rfc.pkl_162.npy',\n",
       " 'rfc.pkl_163.npy',\n",
       " 'rfc.pkl_164.npy',\n",
       " 'rfc.pkl_165.npy',\n",
       " 'rfc.pkl_166.npy',\n",
       " 'rfc.pkl_167.npy',\n",
       " 'rfc.pkl_168.npy',\n",
       " 'rfc.pkl_169.npy',\n",
       " 'rfc.pkl_170.npy',\n",
       " 'rfc.pkl_171.npy',\n",
       " 'rfc.pkl_172.npy',\n",
       " 'rfc.pkl_173.npy',\n",
       " 'rfc.pkl_174.npy',\n",
       " 'rfc.pkl_175.npy',\n",
       " 'rfc.pkl_176.npy',\n",
       " 'rfc.pkl_177.npy',\n",
       " 'rfc.pkl_178.npy',\n",
       " 'rfc.pkl_179.npy',\n",
       " 'rfc.pkl_180.npy',\n",
       " 'rfc.pkl_181.npy',\n",
       " 'rfc.pkl_182.npy',\n",
       " 'rfc.pkl_183.npy',\n",
       " 'rfc.pkl_184.npy',\n",
       " 'rfc.pkl_185.npy',\n",
       " 'rfc.pkl_186.npy',\n",
       " 'rfc.pkl_187.npy',\n",
       " 'rfc.pkl_188.npy',\n",
       " 'rfc.pkl_189.npy',\n",
       " 'rfc.pkl_190.npy',\n",
       " 'rfc.pkl_191.npy',\n",
       " 'rfc.pkl_192.npy',\n",
       " 'rfc.pkl_193.npy',\n",
       " 'rfc.pkl_194.npy',\n",
       " 'rfc.pkl_195.npy',\n",
       " 'rfc.pkl_196.npy',\n",
       " 'rfc.pkl_197.npy',\n",
       " 'rfc.pkl_198.npy',\n",
       " 'rfc.pkl_199.npy',\n",
       " 'rfc.pkl_200.npy',\n",
       " 'rfc.pkl_201.npy']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(classifier, 'model/rfc.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "classifier = joblib.load('model/rfc.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs 1044196\n",
      "Example pair ['0', '5', '4670875']\n"
     ]
    }
   ],
   "source": [
    "test_pairs = read_csv('data/ItemPairs_test.csv', hasHeader=True)\n",
    "print('Train pairs', len(test_pairs))\n",
    "print('Example pair', test_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test items 1315205\n",
      "Item example ['5', '115', 'Сотрудничество салонам кухонной мебели', 'Сотрудничество салонам кухонной мебели.\\nТребуются заказы на ремонт и отделку помещений кухни для установки ВАШЕЙ кухонной мебели.\\nДополнительные вопросы по телефону.', '', '{\"Вид услуги\":\"Другое\"}', '', '637640', '500292.0', '55.760211', '37.577211']\n"
     ]
    }
   ],
   "source": [
    "test_items = read_csv('data/ItemInfo_test.csv', hasHeader=True)\n",
    "print('Test items', len(test_items))\n",
    "print('Item example', test_items[0])\n",
    "\n",
    "map_test_items = dict()\n",
    "for item in test_items:\n",
    "    map_test_items[item[COL_ITEM_ID]] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'13717141.jpg'\n",
      "'12961761.jpg'\n",
      "'12961761.jpg'\n",
      "'12961761.jpg'\n",
      "'12961761.jpg'\n",
      "'9943584.jpg'\n",
      "'12953041.jpg'\n",
      "'4515613.jpg'\n",
      "'13717141.jpg'\n",
      "([1, 1, 0.0, 0, 1, 1, 1, 0.0, 1, 1, 0.25, 0.2, 0, 0], 0)\n"
     ]
    }
   ],
   "source": [
    "def get_test_data(items):\n",
    "    data = []\n",
    "    for pair in items:\n",
    "        item1 = map_test_items[pair[1]]\n",
    "        item2 = map_test_items[pair[2]]\n",
    "\n",
    "        xy = get_features(item1, item2, 0)\n",
    "        data.append(xy)\n",
    "        \n",
    "    return data\n",
    "\n",
    "test = get_test_data(test_pairs)\n",
    "print(test[0])\n",
    "\n",
    "test_f = [x[0] for x in test]\n",
    "\n",
    "test_l = classifier.predict_proba(test_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93811403  0.06188597]\n"
     ]
    }
   ],
   "source": [
    "print(test_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = [(x[0], y[1]) for x,y in zip(test_pairs, test_l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', 0.061885970146551825)\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_csv(filename, header, data):\n",
    "    with open(filename, 'w+') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='\"')\n",
    "        if header is not None:\n",
    "            writer.writerow(header)\n",
    "            \n",
    "        for row in data:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_csv('submission.csv', ['id', 'probability'], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
